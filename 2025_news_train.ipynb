{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, json \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/phuongnguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/phuongnguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/phuongnguyen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func_dataprep import preprocess_text\n",
    "from model_train import CNNModel, GRUModel, LSTMModel, BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset/2025/financial_news_events.json\"\n",
    "# Load data\n",
    "pandas_data = pd.read_json(data_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3024 entries, 0 to 3023\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   Date                  3024 non-null   datetime64[ns]\n",
      " 1   Headline              2876 non-null   object        \n",
      " 2   Source                3024 non-null   object        \n",
      " 3   Market_Event          3024 non-null   object        \n",
      " 4   Market_Index          3024 non-null   object        \n",
      " 5   Index_Change_Percent  2863 non-null   float64       \n",
      " 6   Trading_Volume        3024 non-null   float64       \n",
      " 7   Sentiment             2853 non-null   object        \n",
      " 8   Sector                3024 non-null   object        \n",
      " 9   Impact_Level          3024 non-null   object        \n",
      " 10  Related_Company       3024 non-null   object        \n",
      " 11  News_Url              2871 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(2), object(9)\n",
      "memory usage: 283.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pandas_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date                                           Headline  \\\n",
      "0 2025-05-21        Nikkei 225 index benefits from a weaker yen   \n",
      "1 2025-05-18  Government subsidy program gives a lift to the...   \n",
      "2 2025-06-25  New housing data release shows a slowdown in m...   \n",
      "3 2025-07-21  Massive stock buyback program announced by a c...   \n",
      "4 2025-07-23  Government spending bill is expected to stimul...   \n",
      "\n",
      "                    Source                Market_Event        Market_Index  \\\n",
      "0           Times of India       Commodity Price Shock                 DAX   \n",
      "1          Financial Times        Central Bank Meeting  Shanghai Composite   \n",
      "2  The Hindu Business Line  Consumer Confidence Report  Shanghai Composite   \n",
      "3            The Economist       Commodity Price Shock           NSE Nifty   \n",
      "4          The Motley Fool      Inflation Data Release    Nasdaq Composite   \n",
      "\n",
      "   Index_Change_Percent  Trading_Volume Sentiment          Sector  \\\n",
      "0                  3.52          166.45      None      Technology   \n",
      "1                 -3.39           57.61      None          Retail   \n",
      "2                 -0.05          403.22   Neutral          Retail   \n",
      "3                 -2.29          100.11  Positive  Consumer Goods   \n",
      "4                 -3.97          438.22  Negative  Consumer Goods   \n",
      "\n",
      "  Impact_Level      Related_Company  \\\n",
      "0         High        Goldman Sachs   \n",
      "1          Low           ExxonMobil   \n",
      "2       Medium               Boeing   \n",
      "3          Low  Samsung Electronics   \n",
      "4          Low      JP Morgan Chase   \n",
      "\n",
      "                                            News_Url  \n",
      "0  https://timesofindia.indiatimes.com/business/m...  \n",
      "1  https://timesofindia.indiatimes.com/business/m...  \n",
      "2     https://www.moneycontrol.com/us-markets/sp-500  \n",
      "3  https://www.cnbc.com/2025/09/automotive-indust...  \n",
      "4  https://www.bloomberg.com/australia/asx-200-pe...  \n"
     ]
    }
   ],
   "source": [
    "print(pandas_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: ['Technology' 'Retail' 'Consumer Goods' 'Transportation'\n",
      " 'Telecommunications' 'Energy' 'Pharmaceuticals' 'Healthcare' 'Finance'\n",
      " 'Industrials' 'Real Estate' 'Aerospace & Defense' 'Agriculture'\n",
      " 'Utilities' 'Construction' 'Media & Entertainment' 'Materials'\n",
      " 'Automotive']\n",
      "Length of unique categories: 18\n"
     ]
    }
   ],
   "source": [
    "#unique category\n",
    "categories = pandas_data['Sector'].unique()\n",
    "print(\"Unique categories:\", categories)\n",
    "unique_categories = len(categories)\n",
    "print(\"Length of unique categories:\", unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NaN values of Healine and Sector\n",
    "pandas_data = pandas_data.dropna(subset=['Headline', 'Sector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUTS PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the Headline column\n",
    "pandas_data['input'] = pandas_data['Headline'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            [nikkei, 225, index, benefit, weaker, yen]\n",
      "1     [government, subsidy, program, give, lift, agr...\n",
      "2     [new, housing, data, release, show, slowdown, ...\n",
      "3     [massive, stock, buyback, program, announced, ...\n",
      "4     [government, spending, bill, expected, stimula...\n",
      "5     [central, bank, maintains, status, quo, intere...\n",
      "6      [tech, giant, new, product, launch, spark, gain]\n",
      "7     [massive, data, breach, sends, tech, company, ...\n",
      "9     [market, sentiment, turn, positive, vaccine, t...\n",
      "10    [global, trade, talk, collapse, causing, marke...\n",
      "Name: input, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pandas_data['input'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "all_tokens = [word for tokens in pandas_data['input'] for word in tokens]\n",
    "unique_tokens = list(set(all_tokens))\n",
    "\n",
    "word2idx = {o:i for i,o in enumerate(unique_tokens)}\n",
    "idx2word = {i:o for i,o in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'index', 1: 'requirement', 2: 'oil', 3: 'rattle', 4: 'policy', 5: 'smaller', 6: 'crude', 7: 'tone', 8: 'brace', 9: 'automotive', 10: 'pmi', 11: 'sign', 12: 'plummeting', 13: 'weaker', 14: 'eu', 15: 'boom', 16: 'hint', 17: 'bond', 18: 'expansion', 19: 'recovery', 20: 'governor', 21: 'cut', 22: 'rate', 23: 'benefit', 24: 'deal', 25: 'price', 26: 'streaming', 27: 'ftse', 28: 'activity', 29: 'announced', 30: 'firm', 31: 'sector', 32: 'composite', 33: 'yield', 34: 'estate', 35: 'favorable', 36: 'high', 37: 'shift', 38: 'report', 39: 'drug', 40: 'buyback', 41: 'result', 42: 'impact', 43: 'heavily', 44: 'nikkei', 45: 'slowdown', 46: 'cause', 47: 'inflation', 48: 'revenue', 49: 'cost', 50: 'data', 51: 'give', 52: 'aerospace', 53: 'asx', 54: 'upward', 55: 'interest', 56: 'tension', 57: 'rise', 58: 'restriction', 59: 'outlook', 60: 'overheating', 61: 'jones', 62: 'expectation', 63: 'unemployment', 64: 'utility', 65: 'good', 66: 'dividend', 67: 'potential', 68: 'drop', 69: 'government', 70: 'pharmaceutical', 71: 'excites', 72: 'rival', 73: 'retail', 74: 'two', 75: 'giant', 76: 'housing', 77: 'commodity', 78: 'export', 79: 'chairman', 80: 'decade', 81: 'shortage', 82: 'haven', 83: 'company', 84: 'uk', 85: 'signal', 86: 'shareholder', 87: 'breach', 88: 'weigh', 89: 'collapse', 90: 'earnings', 91: 'central', 92: 'remains', 93: 'drive', 94: 'stimulate', 95: 'gain', 96: 'hike', 97: 'energy', 98: 'confidence', 99: 'announcement', 100: 'maintains', 101: 'increase', 102: 'demand', 103: 'loss', 104: 'issue', 105: 'dovish', 106: 'subsidy', 107: 'robust', 108: 'trial', 109: 'dow', 110: 'ipo', 111: 'geopolitical', 112: 'rally', 113: '100', 114: 'unexpected', 115: 'yen', 116: 'massive', 117: 'federal', 118: 'real', 119: 'riskier', 120: 'service', 121: 'mining', 122: 'future', 123: 'cryptocurrencies', 124: 'reach', 125: 'dip', 126: 'major', 127: 'status', 128: 'pessimism', 129: 'profit', 130: '200', 131: 'face', 132: 'vaccine', 133: 'headwind', 134: 'surprising', 135: 'investor', 136: 'approval', 137: 'industry', 138: 'semiconductor', 139: 'product', 140: 'popular', 141: 'supply', 142: 'material', 143: 'fear', 144: 'reserve', 145: 'talk', 146: 'outperform', 147: 'entertainment', 148: 'china', 149: 'new', 150: 'fall', 151: 'regulation', 152: 'analyst', 153: 'oversubscribed', 154: 'bank', 155: 'trade', 156: 'software', 157: 'tumbling', 158: 'defense', 159: 'fluctuation', 160: 'sale', 161: 'broad', 162: 'program', 163: 'chain', 164: 'stable', 165: 'volatility', 166: 'surpasses', 167: 'shanghai', 168: 'flee', 169: 'beat', 170: 'surge', 171: 'stock', 172: 'seek', 173: 'margin', 174: 'tech', 175: 'release', 176: 'capital', 177: 'plunge', 178: 'sentiment', 179: 'spark', 180: 'rising', 181: 'prompt', 182: 'amid', 183: 'bill', 184: 'safe', 185: '225', 186: 'merger', 187: 'quarterly', 188: 'acquires', 189: 'fuel', 190: 'economic', 191: 'speech', 192: 'show', 193: 'expected', 194: 'healthcare', 195: 'sends', 196: 'turn', 197: 'manufacturing', 198: 'tumble', 199: 'renewable', 200: 'revealed', 201: 'medium', 202: 'low', 203: 'financial', 204: 'positive', 205: 'scandal', 206: 'gold', 207: 'spending', 208: 'strong', 209: 'see', 210: 'banking', 211: 'launch', 212: 'ceasefire', 213: 'agriculture', 214: 'global', 215: 'causing', 216: 'asset', 217: 'discretionary', 218: 'recession', 219: 'spill', 220: 'boost', 221: 'consumer', 222: 'turmoil', 223: 'technology', 224: 'lift', 225: 'market', 226: 'economy', 227: 'bracing', 228: 'dollar', 229: 'forecast', 230: 'performance', 231: 'quo'}\n"
     ]
    }
   ],
   "source": [
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function\n",
    "def map_tokens(tokens, word2idx):\n",
    "    return [word2idx.get(word, 0) for word in tokens]  # 0 nếu từ không có trong vocab\n",
    "\n",
    "# Applied for tokens function\n",
    "pandas_data['indexed'] = pandas_data['input'].apply(lambda tokens: map_tokens(tokens, word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date                                     Headline          Source  \\\n",
      "0 2025-05-21  Nikkei 225 index benefits from a weaker yen  Times of India   \n",
      "\n",
      "            Market_Event Market_Index  Index_Change_Percent  Trading_Volume  \\\n",
      "0  Commodity Price Shock          DAX                  3.52          166.45   \n",
      "\n",
      "  Sentiment      Sector Impact_Level Related_Company  \\\n",
      "0      None  Technology         High   Goldman Sachs   \n",
      "\n",
      "                                            News_Url  \\\n",
      "0  https://timesofindia.indiatimes.com/business/m...   \n",
      "\n",
      "                                        input                    indexed  \n",
      "0  [nikkei, 225, index, benefit, weaker, yen]  [44, 185, 0, 23, 13, 115]  \n"
     ]
    }
   ],
   "source": [
    "print(pandas_data.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that either shorten sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply padding to the input data\n",
    "padded_inputs = pad_input(pandas_data['indexed'], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2876, 30)\n"
     ]
    }
   ],
   "source": [
    "print(padded_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LABELS PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding labels (text > int)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "labels_text = pandas_data['Sector'].values\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "#Fit and transform labels\n",
    "labels_int = encoder.fit_transform(labels_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2876\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aerospace & Defense' 'Agriculture' 'Automotive' 'Construction'\n",
      " 'Consumer Goods' 'Energy' 'Finance' 'Healthcare' 'Industrials'\n",
      " 'Materials' 'Media & Entertainment' 'Pharmaceuticals' 'Real Estate'\n",
      " 'Retail' 'Technology' 'Telecommunications' 'Transportation' 'Utilities']\n"
     ]
    }
   ],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping label dictionary\n",
    "id2label = {i:o for i,o in enumerate(encoder.classes_)}\n",
    "label2id = {o:i for i,o in enumerate(encoder.classes_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to Tensor and create DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, train_labels, test_labels = train_test_split(padded_inputs, labels_int, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = TensorDataset(torch.tensor(train_text, dtype=torch.long), torch.tensor(train_labels, dtype=torch.long))\n",
    "test_data = TensorDataset(torch.tensor(test_text, dtype=torch.long), torch.tensor(test_labels, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "embed_dim = 100\n",
    "num_classes = unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'> 232\n",
      "<class 'int'> 100\n",
      "<class 'int'> 18\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab_size), vocab_size)\n",
    "print(type(embed_dim), embed_dim)\n",
    "print(type(num_classes), num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN = CNNModel(vocab_size, embed_dim, num_classes)\n",
    "modelLSTM = LSTMModel(vocab_size = vocab_size, embed_size=embed_dim, hidden_size=128, output_size =num_classes)\n",
    "modelGRU = GRUModel(input_size = vocab_size, hidden_size=128, output_size =num_classes)\n",
    "modelBiLSTM = BiLSTM(vocab_size, embed_dim, 128, num_classes, 2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelLSTM.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST MODEL WITH TEST INPUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  0,   0,   0,  ..., 122,  22,  21],\n",
      "        [  0,   0,   0,  ..., 164, 225, 159],\n",
      "        [  0,   0,   0,  ..., 174,  31, 129],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ..., 122,  22,  21],\n",
      "        [  0,   0,   0,  ...,  85, 190,  18],\n",
      "        [  0,   0,   0,  ..., 172, 184,  82]]), tensor([15, 14, 14,  2,  5, 14,  8,  0,  7, 14, 12, 12,  4, 14, 12,  9,  4, 14,\n",
      "         2, 13,  3, 16,  5, 10,  5, 16,  1, 13,  5,  9,  7,  4,  9, 17, 17,  3,\n",
      "        16, 11, 15,  8,  9,  3,  0,  7, 17,  3, 16,  5,  6,  0, 17, 12,  0,  9,\n",
      "         7, 14, 12,  0, 17,  8,  6,  8, 13, 17]))\n"
     ]
    }
   ],
   "source": [
    "#Test input data model\n",
    "batch_size = 64\n",
    "input_test = train_data[:batch_size]\n",
    "print(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 18])\n"
     ]
    }
   ],
   "source": [
    "output_test = modelBiLSTM(input_test[0])\n",
    "print(output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "label_test = train_data[:batch_size][1]\n",
    "print(label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test = criterion(output_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17,  7, 13, 10,  0, 12, 13,  6, 16, 16, 17, 10, 10,  2, 11, 10,  2,  6,\n",
      "        10, 13, 15,  8, 16,  3, 12, 11,  6, 10, 17,  8, 10, 16,  8, 16, 10,  6,\n",
      "         7,  0, 13, 17,  0, 11, 16,  2, 13,  3,  0,  6,  6, 11,  3, 11, 11, 17,\n",
      "        11,  0,  0, 13, 13,  0, 17, 11, 11, 10])\n"
     ]
    }
   ],
   "source": [
    "#Test probabilities\n",
    "preds_test = torch.argmax(output_test, dim=1)\n",
    "print(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities test tensor([[0.0537, 0.0511, 0.0570,  ..., 0.0537, 0.0595, 0.0649],\n",
      "        [0.0564, 0.0529, 0.0532,  ..., 0.0547, 0.0556, 0.0590],\n",
      "        [0.0595, 0.0525, 0.0565,  ..., 0.0545, 0.0528, 0.0570],\n",
      "        ...,\n",
      "        [0.0579, 0.0514, 0.0560,  ..., 0.0558, 0.0550, 0.0572],\n",
      "        [0.0573, 0.0540, 0.0585,  ..., 0.0534, 0.0559, 0.0571],\n",
      "        [0.0559, 0.0512, 0.0559,  ..., 0.0572, 0.0557, 0.0601]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "predictions test tensor([17,  7, 13, 10,  0, 12, 13,  6, 16, 16, 17, 10, 10,  2, 11, 10,  2,  6,\n",
      "        10, 13, 15,  8, 16,  3, 12, 11,  6, 10, 17,  8, 10, 16,  8, 16, 10,  6,\n",
      "         7,  0, 13, 17,  0, 11, 16,  2, 13,  3,  0,  6,  6, 11,  3, 11, 11, 17,\n",
      "        11,  0,  0, 13, 13,  0, 17, 11, 11, 10])\n"
     ]
    }
   ],
   "source": [
    "#Test probabilities\n",
    "probs_test = F.softmax(output_test, dim=-1)\n",
    "print('probabilities test', probs_test)\n",
    "preds_test = torch.argmax(probs_test, dim=-1)\n",
    "print('predictions test', preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config Model before training\n",
    "model = modelBiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.04528419950734014\n",
      "Epoch 2, Training Loss: 0.045227501392364505\n",
      "Epoch 3, Training Loss: 0.045284293838169266\n",
      "Epoch 4, Training Loss: 0.04529779859211134\n",
      "Epoch 5, Training Loss: 0.045263649173404856\n",
      "Epoch 6, Training Loss: 0.045264561590941055\n",
      "Epoch 7, Training Loss: 0.04525750222413436\n",
      "Epoch 8, Training Loss: 0.04527691115503726\n",
      "Epoch 9, Training Loss: 0.04527451338975326\n",
      "Epoch 10, Training Loss: 0.04525009797966999\n"
     ]
    }
   ],
   "source": [
    "#Training epoch\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "    epoch_loss = training_loss/ len(train_data)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valiation  \n",
    "validation_loss = 0.0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        validation_loss += loss.item()\n",
    "epoch_val_loss = validation_loss / len(test_loader)\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.892735905117459\n"
     ]
    }
   ],
   "source": [
    "print(epoch_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.0451, Precision=0.0135, Recall=0.0451\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall\n",
    "\n",
    "num_classes = unique_categories\n",
    "accuracy = MulticlassAccuracy(num_classes=num_classes, average=\"weighted\").to(\"cpu\")\n",
    "precision = MulticlassPrecision(num_classes=num_classes, average=\"weighted\").to(\"cpu\")\n",
    "recall = MulticlassRecall(num_classes=num_classes, average=\"weighted\").to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(\"cpu\"), labels.to(\"cpu\")\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        # Update metrics each batch\n",
    "        accuracy.update(outputs, labels)\n",
    "        precision.update(outputs, labels)\n",
    "        recall.update(outputs, labels)\n",
    "\n",
    "# Compute metrics after going through the full dataloader\n",
    "acc = accuracy.compute().item()\n",
    "prec = precision.compute().item()\n",
    "rec = recall.compute().item()\n",
    "\n",
    "print(f\"Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}\")\n",
    "\n",
    "# Reset for next evaluation\n",
    "accuracy.reset()\n",
    "precision.reset()\n",
    "recall.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET PREDICTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
